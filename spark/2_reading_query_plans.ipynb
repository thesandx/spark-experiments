{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YT Video link](https://youtu.be/KnUXztKueMU?si=ILuFJgT4CVukrOMz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Query Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### understanding spark query plan (with examples)\n",
    "\n",
    "when you write a spark query, it doesn't just run directly. spark goes through multiple stages to transform your code into an optimized execution plan. let's break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## **step 1: writing pyspark code**\n",
    "consider this simple query:\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "df.filter(df[\"salary\"] > 50000).select(\"name\", \"salary\").show()\n",
    "```\n",
    "\n",
    "even though we wrote just two operations (`filter` and `select`), under the hood, spark follows **several steps** before execution.\n",
    "\n",
    "---\n",
    "\n",
    "## **step 2: syntax validation**\n",
    "before spark even thinks about execution, it checks whether the code **follows correct syntax** (like checking if columns exist, functions are used properly, etc.).\n",
    "\n",
    "- ✅ `df[\"salary\"] > 50000` → valid syntax  \n",
    "- ❌ `df[\"salary\"] > \"high\"` → invalid, type mismatch error\n",
    "\n",
    "if the syntax is fine, spark moves ahead.\n",
    "\n",
    "---\n",
    "\n",
    "## **step 3: unresolved logical plan**\n",
    "at first, spark **doesn’t know what `salary` means** because it hasn’t checked the dataset yet. it creates an **unresolved logical plan**, meaning:\n",
    "\n",
    "- it understands the structure of the query (`filter` then `select`)\n",
    "- but it doesn't know if the columns exist or what their types are\n",
    "\n",
    "### **example of an unresolved logical plan**\n",
    "```plaintext\n",
    "'Project ['name, 'salary]   ← selecting columns\n",
    " +- 'Filter ('salary > 50000)   ← filtering condition\n",
    "    +- 'UnresolvedRelation [employees.csv]   ← table not verified yet\n",
    "```\n",
    "\n",
    "at this stage, `salary` and `employees.csv` are just **strings**, not actual references.\n",
    "\n",
    "---\n",
    "\n",
    "## **step 4: catalog lookup (resolving logical plan)**\n",
    "spark now **consults the catalog** (metadata store) to **validate the table and column names**.\n",
    "\n",
    "- ✅ if `employees.csv` is found and contains `salary` → spark proceeds  \n",
    "- ❌ if the table or column is missing → error: `AnalysisException: cannot resolve column 'salary'`\n",
    "\n",
    "once resolved, the logical plan is updated with **actual references** instead of just names.\n",
    "\n",
    "### **resolved logical plan**\n",
    "```plaintext\n",
    "Project [name, salary]\n",
    " +- Filter (salary#23 > 50000)\n",
    "    +- Relation[ name#21, salary#23] csv\n",
    "```\n",
    "\n",
    "now, `salary#23` means it's a real column from the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **step 5: logical optimization (catalyst optimizer)**\n",
    "spark now **optimizes the logical plan** using **rule-based optimizations**.\n",
    "\n",
    "### **example optimization - predicate pushdown**\n",
    "consider if our `employees.csv` is huge. instead of reading everything, spark **pushes the filter down** to the data source level (if supported).\n",
    "\n",
    "#### **before optimization (bad)**\n",
    "```plaintext\n",
    "Read entire employees.csv → Apply filter (salary > 50000) → Select columns\n",
    "```\n",
    "(spark reads all data first, which is inefficient.)\n",
    "\n",
    "#### **after optimization (good)**\n",
    "```plaintext\n",
    "Push filter (salary > 50000) to csv reader → Read only matching rows → Select columns\n",
    "```\n",
    "(this reduces data read from disk, making execution faster.)\n",
    "\n",
    "this is called **filter pushdown**, and spark does similar optimizations like **pushdown projection**, where it only reads required columns instead of all.\n",
    "\n",
    "### **optimized logical plan**\n",
    "```plaintext\n",
    "Project [name, salary]\n",
    " +- Relation[ name#21, salary#23] csv (PushedFilters: [salary > 50000])\n",
    "```\n",
    "(the filter has been pushed down to the csv reader.)\n",
    "\n",
    "---\n",
    "\n",
    "## **step 6: physical plan generation**\n",
    "now spark converts the optimized logical plan into an **actual execution plan**.\n",
    "\n",
    "- **what execution strategy should be used?**\n",
    "- **what is the best way to run this on a distributed cluster?**\n",
    "\n",
    "this stage decides whether to:\n",
    "- use **sort-merge join** or **broadcast join**\n",
    "- use **hash aggregation** or **sort aggregation**\n",
    "- apply **parallel execution strategies**\n",
    "\n",
    "### **example of a physical plan**\n",
    "```plaintext\n",
    "*(1) Project [name, salary]\n",
    " +- *(1) Filter (salary#23 > 50000)\n",
    "    +- FileScan csv [name#21, salary#23] PushedFilters: [salary > 50000]\n",
    "```\n",
    "- `*(1)` → means it's **executing on multiple cores in parallel**\n",
    "- `FileScan csv` → shows it's reading only **necessary columns and rows**\n",
    "- `PushedFilters: [salary > 50000]` → confirms **filter pushdown happened**\n",
    "\n",
    "---\n",
    "\n",
    "## **step 7: execution**\n",
    "now spark **executes the physical plan** in stages:\n",
    "1. loads the data (with filters applied at the source if possible)\n",
    "2. applies transformations in a distributed way\n",
    "3. materializes results and returns output\n",
    "\n",
    "```\n",
    "+-------+--------+\n",
    "|  name | salary |\n",
    "+-------+--------+\n",
    "| Alice | 60000  |\n",
    "|  Bob  | 70000  |\n",
    "+-------+--------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **summary of spark query plan stages**\n",
    "| stage | what happens? |\n",
    "|-------|--------------|\n",
    "| **1. syntax validation** | checks if the query syntax is correct |\n",
    "| **2. unresolved logical plan** | spark understands the structure but doesn’t verify column names or table existence |\n",
    "| **3. catalog lookup (resolving)** | spark verifies column names & table existence |\n",
    "| **4. logical optimization (catalyst optimizer)** | optimizes query, applies filter & projection pushdown |\n",
    "| **5. physical plan generation** | decides best execution strategy (joins, aggregations, etc.) |\n",
    "| **6. execution** | spark runs the plan in parallel on the cluster |\n",
    "\n",
    "---\n",
    "\n",
    "## **extra: seeing the query plan in action**\n",
    "you can inspect the query plan using:\n",
    "```python\n",
    "df.explain(True)  # detailed plan\n",
    "```\n",
    "or\n",
    "```python\n",
    "df.explain(\"formatted\")  # better readability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **key optimizations in catalyst optimizer**\n",
    "1. **predicate pushdown**: apply `WHERE` filters at the data source (reducing I/O)\n",
    "2. **projection pushdown**: only select required columns (avoiding reading extra data)\n",
    "3. **constant folding**: precompute constant expressions (`2 + 3 → 5`)\n",
    "4. **reordering joins**: reorders joins to process smaller datasets first\n",
    "5. **eliminating unnecessary operations**: removes redundant transformations\n",
    "\n",
    "---\n",
    "\n",
    "## **final thoughts**\n",
    "- spark **does not execute line by line**—it **builds a plan, optimizes it, then executes efficiently**.\n",
    "- **filter pushdown** and **projection pushdown** reduce data read from disk, improving performance.\n",
    "- `explain()` is your friend—**use it to understand query plans** and identify inefficiencies.\n",
    "\n",
    "---\n",
    "\n",
    "that’s the full breakdown. lmk if you need more depth on any step. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MyApp').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"../data/data_skew/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city       |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|10   |7  |Entertainment|10.42 |boston     |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|3    |27 |Motor/Travel |44.34 |portland   |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|4    |11 |Entertainment|3.18  |chicago    |\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|2    |22 |Groceries    |268.97|los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|10   |16 |Entertainment|2.66  |chicago    |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5, False) #show full string, no tructacate(True i.e truncate by dfeault value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = \"../data/data_skew/customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city       |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n",
      "+----------+-------------+---+------+----------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spark's Query Plans\n",
    "\n",
    "Image Credits: `Databricks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Execution](../data/spark-execution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In PySpark, lit(5) creates a column with the literal value 5 for every row in a DataFrame. The lit function is used to add a column with a constant value, regardless of the other data in the DataFrame. It's useful for tasks like adding flags, metadata, or performing calculations with a fixed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+---+------+---------+\n",
      "|cust_id   |first_name|last_name|age|gender|birthday |\n",
      "+----------+----------+---------+---+------+---------+\n",
      "|C007YEYTX9|Aaron     |Abbott   |39 |Female|7/13/1991|\n",
      "|C08XAQUY73|Aaron     |Lambert  |59 |Female|11/5/1966|\n",
      "|C094P1VXF9|Aaron     |Lindsey  |29 |Male  |9/21/1990|\n",
      "|C097SHE1EF|Aaron     |Lopez    |27 |Female|4/18/2001|\n",
      "|C0DTC6436T|Aaron     |Schwartz |57 |Female|7/9/1962 |\n",
      "+----------+----------+---------+---+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['cust_id, 'first_name, 'last_name, 'age, 'gender, 'birthday]\n",
      "+- Project [cust_id#22, name#23, (cast(age#24 as bigint) + cast(5 as bigint)) AS age#131L, gender#25, birthday#26, zip#27, city#28, first_name#112, last_name#121]\n",
      "   +- Project [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28, first_name#112, split(name#23,  , -1)[1] AS last_name#121]\n",
      "      +- Project [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28, split(name#23,  , -1)[0] AS first_name#112]\n",
      "         +- Filter (city#28 = boston)\n",
      "            +- Relation [cust_id#22,name#23,age#24,gender#25,birthday#26,zip#27,city#28] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, first_name: string, last_name: string, age: bigint, gender: string, birthday: string\n",
      "Project [cust_id#22, first_name#112, last_name#121, age#131L, gender#25, birthday#26]\n",
      "+- Project [cust_id#22, name#23, (cast(age#24 as bigint) + cast(5 as bigint)) AS age#131L, gender#25, birthday#26, zip#27, city#28, first_name#112, last_name#121]\n",
      "   +- Project [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28, first_name#112, split(name#23,  , -1)[1] AS last_name#121]\n",
      "      +- Project [cust_id#22, name#23, age#24, gender#25, birthday#26, zip#27, city#28, split(name#23,  , -1)[0] AS first_name#112]\n",
      "         +- Filter (city#28 = boston)\n",
      "            +- Relation [cust_id#22,name#23,age#24,gender#25,birthday#26,zip#27,city#28] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#22, split(name#23,  , -1)[0] AS first_name#112, split(name#23,  , -1)[1] AS last_name#121, (cast(age#24 as bigint) + 5) AS age#131L, gender#25, birthday#26]\n",
      "+- Filter (isnotnull(city#28) AND (city#28 = boston))\n",
      "   +- Relation [cust_id#22,name#23,age#24,gender#25,birthday#26,zip#27,city#28] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#22, split(name#23,  , -1)[0] AS first_name#112, split(name#23,  , -1)[1] AS last_name#121, (cast(age#24 as bigint) + 5) AS age#131L, gender#25, birthday#26]\n",
      "+- *(1) Filter (isnotnull(city#28) AND (city#28 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [cust_id#22,name#23,age#24,gender#25,birthday#26,city#28] Batched: true, DataFilters: [isnotnull(city#28), (city#28 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/spark-experiments/data/data_skew/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_narrow_transform = (\n",
    "    df_customers\n",
    "    .filter(col(\"city\") == \"boston\")\n",
    "    .withColumn(\"first_name\", split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", col(\"age\") + lit(5))\n",
    "    .select(\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\")\n",
    ")\n",
    "\n",
    "df_narrow_transform.show(5, False)\n",
    "df_narrow_transform.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Wide Transformations\n",
    "1. Repartition\n",
    "2. Coalesce\n",
    "3. Joins\n",
    "4. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(24), REPARTITION_BY_NUM, [plan_id=80]\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/spark-experiments/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.repartition(24).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 5, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 5\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(5).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why doesn't `.coalesce()` explicitly show the partitioning scheme?\n",
    "\n",
    "`.coalesce` doesn't show the partitioning scheme e.g. `RoundRobinPartitioning` because: \n",
    "- The operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling.\n",
    "- Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by `.coalesce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conset(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(cust_id))\n",
      ":- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "+- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "+- Join Inner, (cust_id#0 = cust_id#67)\n",
      "   :- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "+- Join Inner, (cust_id#0 = cust_id#67)\n",
      "   :- Filter isnotnull(cust_id#0)\n",
      "   :  +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Filter isnotnull(cust_id#67)\n",
      "      +- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#68, age#69, gender#70, birthday#71, zip#72, city#73]\n",
      "   +- SortMergeJoin [cust_id#0], [cust_id#67], Inner\n",
      "      :- Sort [cust_id#0 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [id=#114]\n",
      "      :     +- Filter isnotnull(cust_id#0)\n",
      "      :        +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [isnotnull(cust_id#0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "      +- Sort [cust_id#67 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#67, 200), ENSURE_REQUIREMENTS, [id=#115]\n",
      "            +- Filter isnotnull(cust_id#67)\n",
      "               +- FileScan parquet [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] Batched: true, DataFilters: [isnotnull(cust_id#67)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, count(1) AS count#199L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, count: bigint\n",
      "Aggregate [city#10], [city#10, count(1) AS count#199L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, count(1) AS count#199L]\n",
      "+- Project [city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[count(1)], output=[city#10, count#199L])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [id=#131]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_count(1)], output=[city#10, count#203L])\n",
      "         +- FileScan parquet [city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_counts.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(sum(\"amt\").alias(\"txn_amt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, sum('amt) AS txn_amt#216]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, txn_amt: double\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#216]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS txn_amt#216]\n",
      "+- Project [amt#9, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[sum(cast(amt#9 as double))], output=[city#10, txn_amt#216])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [id=#144]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_sum(cast(amt#9 as double))], output=[city#10, sum#220])\n",
      "         +- FileScan parquet [amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<amt:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txn_amt_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Count Distinct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"cust_id\")\n",
    "    .agg(countDistinct(\"city\").alias(\"city_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|cust_id   |city_count|\n",
      "+----------+----------+\n",
      "|CPP8BY8U93|10        |\n",
      "|CYB8BX9LU1|10        |\n",
      "|CFRT841CCD|10        |\n",
      "|CA0TSNMYDK|10        |\n",
      "|COZ8NONEVZ|10        |\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['cust_id], ['cust_id, 'count(distinct 'city) AS city_count#232]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, city_count: bigint\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS city_count#232L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS city_count#232L]\n",
      "+- Project [cust_id#0, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[cust_id#0], functions=[count(distinct city#10)], output=[cust_id#0, city_count#232L])\n",
      "   +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [id=#266]\n",
      "      +- HashAggregate(keys=[cust_id#0], functions=[partial_count(distinct city#10)], output=[cust_id#0, count#244L])\n",
      "         +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "            +- Exchange hashpartitioning(cust_id#0, city#10, 200), ENSURE_REQUIREMENTS, [id=#262]\n",
      "               +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "                  +- FileScan parquet [cust_id#0,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,city:string>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_txn_per_city.show(5, False)\n",
    "df_txn_per_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Intresting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is a filter step present despite predicate pushdown? \n",
    "\n",
    "This is largely due to the way `Spark's Catalyst Optimizer` works. Specifically, due to two separate stages of the query optimization process: Physical Planning and Logical Planning.\n",
    "\n",
    "- **Logical Planning**: Catalyst optimizer simplifies the unresolved logical plan (which represents the user's query) by applying various rule-based optimizations. This includes `predicate pushdown`, `projection pushdown` where filter conditions and column projections are moved as close to the data source as possible.\n",
    "\n",
    "- **Physical Planning** phase is where the logical plan is translated into one or more physical plans, which can actually be executed on the cluster. This includes operations like file `scans`, `filters`, `projections`, etc.\n",
    "\n",
    "In this case, during the logical planning phase, the predicate (`col(\"city\") == \"boston\"`) has been pushed down and will be applied during the scan of the Parquet file (`PushedFilters: [IsNotNull(city), EqualTo(city,boston)]`), thus improving performance.\n",
    "\n",
    "Now, during the physical planning phase, the same filter condition (`+- *(1) Filter (isnotnull(city#73) AND (city#73 = boston))`) is applied again to the data that's been loaded into memory. This is because of the following reasons:\n",
    "\n",
    "1. **Guaranteed Correctness:** It might seem **redundant**, but remember that not all data sources can handle pushed-down predicates, and not all predicates can be pushed down. Therefore, **even if a predicate is pushed down to the data source, Spark still includes the predicate in the physical plan** to cover cases where the data source might not have been able to fully apply the predicate. This is Spark's way of making sure the correct data is always returned, no matter the capabilities of the data source.\n",
    "\n",
    "2. **No Assumptions**: Spark's Catalyst optimizer doesn't make assumptions about the data source's ability to handle pushed-down predicates. The optimizer aims to generate plans that return correct results across a wide range of scenarios. Even if the filter is pushed down, Spark does not have the feedback from data source whether the pushdown was successful or not, so it includes the filter operation in the physical plan as well.\n",
    "\n",
    "It is more of a **fail-safe mechanism** to ensure data **integrity** and **correctness**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what cases will predicate pushdown not work?\n",
    "\n",
    "2 Examples where **filter pushdown** will not work:\n",
    "\n",
    "1. **Complex Data Types**: Spark's Parquet data source does not push down filters that involve **complex types**, such as **arrays**, **maps**, and **structs**. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    "\n",
    "+----------+-----------------------------+\n",
    "|Name      |properties                   |\n",
    "+----------+-----------------------------+\n",
    "|Afaque    |[eye -> black, hair -> black]|\n",
    "|Naved     |[eye ->, hair -> brown]      |\n",
    "|Ali       |[eye -> black, hair -> red]  |\n",
    "|Amaan     |[eye -> grey, hair -> grey]  |\n",
    "|Omaira    |[eye -> , hair -> brown]     |\n",
    "+----------+-----------------------------+\n",
    "```\n",
    "\n",
    "```python\n",
    "dfilter(dproperties.getItem(\"eye\") == \"brown\").show()\n",
    "```\n",
    "\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) Filter (metadata#123[key] = value)\n",
    "+- *(1) ColumnarToRow\n",
    "   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n",
    "```\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "3. Unsupported Expressions: \n",
    "\n",
    "In Spark, `Parquet` data source does not support pushdown for filters involving a `.cast` operation. The reason for this behaviour is as follows:\n",
    "- `.cast` changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.\n",
    "\n",
    "**Note**: This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the `.cast` filter could potentially be pushed down to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of operation where filter pushdown doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|cust_id   |name          |age|gender|birthday  |zip  |city        |\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|C01BKUFRHA|Aaron Becker  |54 |Male  |11/24/1979|40284|san_diego   |\n",
      "|C01WMZQ7PN|Aaron Brady   |51 |Female|8/20/1994 |52204|philadelphia|\n",
      "|C021567NJZ|Aaron Briggs  |57 |Male  |3/10/1990 |22008|philadelphia|\n",
      "|C02JNTM46B|Aaron Chambers|51 |Male  |1/6/2001  |63337|new_york    |\n",
      "|C030A69V1L|Aaron Clarke  |55 |Male  |4/28/1999 |77176|philadelphia|\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Filter (cast('age as int) > 50)\n",
      "+- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Filter (cast(age#69 as int) > 50)\n",
      "+- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(age#69) AND (cast(age#69 as int) > 50))\n",
      "+- Relation [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(age#69) AND (cast(age#69 as int) > 50))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [cust_id#67,name#68,age#69,gender#70,birthday#71,zip#72,city#73] Batched: true, DataFilters: [isnotnull(age#69), (cast(age#69 as int) > 50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/youtube/spark-experiments/data/data_..., PartitionFilters: [], PushedFilters: [IsNotNull(age)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer_gt_50 = (\n",
    "    df_customers\n",
    "    .filter(col(\"age\").cast(\"int\") > 50)\n",
    ")\n",
    "df_customer_gt_50.show(5, False)\n",
    "df_customer_gt_50.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understanding exchange, roundrobinpartitioning, hashpartitioning, and hashaggregate in spark query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **understanding `exchange`, `roundrobinpartitioning`, `hashpartitioning`, and `hashaggregate` in spark query plans**  \n",
    "\n",
    "when you see **exchange** in a spark query plan, it means **data is being shuffled** across partitions. let’s break down what this means and why it happens.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. what is exchange? (data shuffle)**\n",
    "`exchange` means spark is **redistributing data across partitions**, usually because of a join, groupBy, or some other operation that requires rebalancing data.\n",
    "\n",
    "- **before exchange**: data is in some initial partitions\n",
    "- **exchange happens**: spark moves data around the cluster\n",
    "- **after exchange**: data is now in new partitions\n",
    "\n",
    "**example:**  \n",
    "```python\n",
    "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.groupBy(\"department\").count().explain(True)\n",
    "```\n",
    "**query plan output:**\n",
    "```plaintext\n",
    "== Physical Plan ==\n",
    "*(2) HashAggregate(keys=[department#23], functions=[count(1)])\n",
    "+- Exchange hashpartitioning(department#23, 200), REPARTITION_BY_NUM\n",
    "   +- *(1) HashAggregate(keys=[department#23], functions=[partial_count(1)])\n",
    "      +- FileScan csv [department#23]\n",
    "```\n",
    "### **what’s happening here?**\n",
    "- **`FileScan csv`** → loads data\n",
    "- **`HashAggregate (partial_count)`** → counts records **locally** on each partition\n",
    "- **`Exchange hashpartitioning(department, 200)`** → **shuffles data** so that all rows with the same `department` end up on the same partition\n",
    "- **`HashAggregate (final count)`** → performs final aggregation after shuffle\n",
    "\n",
    "---\n",
    "\n",
    "## **2. `roundrobinpartitioning` (fair redistribution)**\n",
    "this partitioning method **evenly distributes rows across partitions in a round-robin fashion**. it **ignores keys** and just tries to balance load.\n",
    "\n",
    "**use case:**  \n",
    "- when reading large datasets and you want balanced partitions (e.g., before training an ML model)\n",
    "\n",
    "**example:**  \n",
    "```python\n",
    "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "df.repartition(10).explain(True)\n",
    "```\n",
    "**query plan output:**\n",
    "```plaintext\n",
    "== Physical Plan ==\n",
    "Exchange roundrobinpartitioning(10), REPARTITION_BY_NUM\n",
    "+- FileScan csv [name#21, department#23, salary#25]\n",
    "```\n",
    "### **what’s happening here?**\n",
    "- **`FileScan csv`** → reads the file\n",
    "- **`Exchange roundrobinpartitioning(10)`** → reshuffles data evenly across **10 partitions**  \n",
    "\n",
    "**why use this?**\n",
    "- good when **no natural partitioning key exists** but you want balanced partitions\n",
    "- not efficient for **joins or aggregations** since it ignores data relationships\n",
    "\n",
    "---\n",
    "\n",
    "## **3. `hashpartitioning` (grouping similar data together)**\n",
    "this partitioning method **distributes data based on a hash of a column value**.  \n",
    "\n",
    "**use case:**  \n",
    "- used in **joins** and **groupBys** where we need the same key in the same partition  \n",
    "\n",
    "**example:**  \n",
    "```python\n",
    "df1 = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df1.join(df2, \"department\").explain(True)\n",
    "```\n",
    "**query plan output:**\n",
    "```plaintext\n",
    "== Physical Plan ==\n",
    "*(5) SortMergeJoin [department#23], [department#44], Inner\n",
    ":- Exchange hashpartitioning(department#23, 200), REPARTITION_BY_NUM\n",
    "+- *(2) Scan csv [department#23, name#21]\n",
    "+- Exchange hashpartitioning(department#44, 200), REPARTITION_BY_NUM\n",
    "   +- *(4) Scan csv [department#44, location#45]\n",
    "```\n",
    "### **what’s happening here?**\n",
    "- `Scan csv` → reads both datasets\n",
    "- `Exchange hashpartitioning(department, 200)` → **shuffles data so that all rows with the same department go to the same partition**\n",
    "- `SortMergeJoin` → now joins can happen efficiently since related rows are co-located\n",
    "\n",
    "**why use this?**\n",
    "- avoids **full table scans** by ensuring related rows are in the same partition\n",
    "- reduces **network traffic** during joins\n",
    "\n",
    "---\n",
    "\n",
    "## **4. `hashaggregate` (optimized aggregation)**\n",
    "this is an **optimized aggregation technique** that avoids full shuffle when possible.\n",
    "\n",
    "**example:**  \n",
    "```python\n",
    "df.groupBy(\"department\").agg({\"salary\": \"sum\"}).explain(True)\n",
    "```\n",
    "**query plan output:**\n",
    "```plaintext\n",
    "== Physical Plan ==\n",
    "*(2) HashAggregate(keys=[department#23], functions=[sum(salary#25)])\n",
    "+- Exchange hashpartitioning(department#23, 200), REPARTITION_BY_NUM\n",
    "   +- *(1) HashAggregate(keys=[department#23], functions=[partial_sum(salary#25)])\n",
    "      +- FileScan csv [department#23, salary#25]\n",
    "```\n",
    "### **what’s happening here?**\n",
    "1. **`HashAggregate (partial_sum)`** → spark first **partially** sums values locally\n",
    "2. **`Exchange hashpartitioning(department, 200)`** → shuffles data so that all `department` rows are together\n",
    "3. **`HashAggregate (final sum)`** → computes the **final sum** after shuffling\n",
    "\n",
    "**why use this?**\n",
    "- faster than naive aggregation (e.g., sort-based aggregation)\n",
    "- avoids **unnecessary sorting**, which saves time\n",
    "\n",
    "---\n",
    "\n",
    "## **summary of key concepts**\n",
    "| term | what it does | when is it used? |\n",
    "|------|-------------|------------------|\n",
    "| **exchange** | shuffles data across partitions | joins, groupBy, repartition |\n",
    "| **roundrobinpartitioning** | distributes rows evenly across partitions (ignoring keys) | load balancing, ML preprocessing |\n",
    "| **hashpartitioning** | distributes rows based on a hash of a key column | joins, groupBy, aggregations |\n",
    "| **hashaggregate** | optimizes aggregations by avoiding full shuffle | sum, count, avg, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "## **final thoughts**\n",
    "- `exchange` = **shuffle**, which can be expensive, so minimize it\n",
    "- `roundrobinpartitioning` = **balance load** but **not good for joins**\n",
    "- `hashpartitioning` = **good for joins and aggregations** (avoids full table scans)\n",
    "- `hashaggregate` = **faster aggregation without sorting**\n",
    "\n",
    "if you want to avoid excessive shuffling, **always check `df.explain(True)`** to understand what spark is doing. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video Summary\n",
    "\n",
    "Here are the notes from the YouTube video \"Master Reading Spark Query Plans\":\n",
    "*   It is crucial to understand Spark query plans before doing any kind of Spark optimization.\n",
    "*   Spark query plans help in understanding how Spark functions internally.\n",
    "\n",
    "**Spark Query Plan Generation**\n",
    "*   Spark checks the syntax of the code.\n",
    "*   Spark generates an unresolved logical plan.\n",
    "*   Spark uses a catalog to maintain details about tables, data frames, columns, and data types.\n",
    "*   Spark verifies the existence of tables and columns.\n",
    "*   Spark generates a logical plan.\n",
    "*   The Catalyst Optimizer optimizes the logical plan.\n",
    "*   **Filter push down** optimization pushes filters to the data source.\n",
    "*   **Push down projection** optimization selects only the columns used in the query.\n",
    "*   An optimized logical plan is converted into several physical plans.\n",
    "*   Physical plans run on the cluster.\n",
    "*   An internal cost model chooses the most efficient physical plan.\n",
    "*   The final query plan is executed on the cluster.\n",
    "\n",
    "**Narrow Transformations**\n",
    "*   Narrow transformations are operations that don't involve a shuffle. Examples: filter operation, adding a new column, or selecting columns.\n",
    "*   The physical plan is the one executed on the cluster.\n",
    "*   The steps in the physical plan include file scan, columnar to row conversion, filter operation, and project.\n",
    "*   Spark adds \"is not null\" checks as part of its optimization.\n",
    "\n",
    "**Wide Transformations**\n",
    "*   Wide transformations involve shuffle.\n",
    "*   **Repartitioning** redistributes data to a specified number of partitions.\n",
    "*   Repartitioning involves shuffling data across partitions.\n",
    "*   **Round Robin partitioning** is a partitioning scheme that distributes data row by row to each partition.\n",
    "*   **Adaptive Query Execution (AQE)** uses runtime statistics to select the most efficient query plan.\n",
    "*   AQE considers runtime statistics like the number of byte rate and number of partitions.\n",
    "\n",
    "**Coalesce**\n",
    "*   Coalesce helps reduce the number of partitions.\n",
    "*   Coalesce tries to avoid a shuffle unless it has to do it in some extreme edge cases.\n",
    "*   Coalesce merges partitions within the same executor to avoid data transfer between executors.\n",
    "*   In cases of aggressive reduction in the number of partitions, coalesce may involve shuffling.\n",
    "*   When repartitioning, a partitioning scheme is involved (e.g., round robin partitioning).\n",
    "*   When doing a coalesce, there is no partitioning scheme involved because there is no shuffle.\n",
    "\n",
    "**Join**\n",
    "*   The query plan is read from bottom to top.\n",
    "*   The steps in the physical plan for a join include file scan, filter (not null), shuffle, sort, and sort merge join.\n",
    "*   **Hash partitioning** ensures that the same keys end up in the same partition.\n",
    "*   Hash partitioning involves hashing the join key and using the modulo operator to determine the partition.\n",
    "\n",
    "**Group By**\n",
    "*   The physical plan for a group by operation includes file read and hash aggregate.\n",
    "*   Hash aggregate involves doing a local count before shuffling the data.\n",
    "*   After shuffling, the same keys end up in the same partition.\n",
    "*   A final count is performed after the shuffle.\n",
    "*   When counting distinct values, the query plan involves multiple hash aggregates and shuffles.\n",
    "*   The distinct count is performed at the local level before shuffling and then at the global level.\n",
    "\n",
    "**Observations on Narrow Transformation**\n",
    "*   Spark adds a filter step for the filter added in the code.\n",
    "*   Spark pushes filters to the source during the file scan.\n",
    "*   Spark puts another filter to guarantee correctness.\n",
    "*   Operating on a smaller dataset after filtering at the start makes the subsequent filter operation less redundant.\n",
    "*   Predicate pushdown may not work when the column is a map type or when filtering on an unsupported expression (e.g., a Cast Operation).\n",
    "*   Whether filters can be pushed down depends on the data source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCQ Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some multiple-choice questions (MCQs) based on the concepts from the YouTube video \"Master Reading Spark Query Plans\" to help you revise:\n",
    "\n",
    "1.  Which of the following is the **first step** in Spark query plan generation, according to the video?\n",
    "    *   A) Generating physical plans\n",
    "    *   B) Checking the syntax of the code\n",
    "    *   C) Optimizing the logical plan\n",
    "    *   D) Executing the final query plan\n",
    "\n",
    "2.  What is the purpose of the **Catalyst Optimizer** in Spark query plan generation?\n",
    "    *   A) To check the syntax of the code\n",
    "    *   B) To maintain details about tables and data frames\n",
    "    *   C) To optimize the logical plan\n",
    "    *   D) To execute the final query plan\n",
    "\n",
    "3.  Which of the following is an example of a **narrow transformation** in Spark?\n",
    "    *   A) Repartitioning\n",
    "    *   B) Shuffling\n",
    "    *   C) Adding a new column\n",
    "    *   D) Coalesce with aggressive reduction\n",
    "\n",
    "4.  What does **\"filter push down\"** optimization do?\n",
    "    *   A) Selects only the columns used in the query\n",
    "    *   B) Pushes filters to the data source\n",
    "    *   C) Redistributes data to a specified number of partitions\n",
    "    *   D) Merges partitions within the same executor\n",
    "\n",
    "5.  Which of the following is **NOT** a characteristic of **coalesce**?\n",
    "    *   A) It helps reduce the number of partitions\n",
    "    *   B) It tries to avoid a shuffle\n",
    "    *   C) It always involves a partitioning scheme\n",
    "    *   D) It merges partitions within the same executor\n",
    "\n",
    "6.  What does **Adaptive Query Execution (AQE)** use to select the most efficient query plan?\n",
    "    *   A) Syntax of the code\n",
    "    *   B) Logical plan\n",
    "    *   C) Runtime statistics\n",
    "    *   D) Physical plan\n",
    "\n",
    "7.  In a Spark query plan for a join operation, what does **hash partitioning** ensure?\n",
    "    *   A) That the data is sorted\n",
    "    *   B) That null values are filtered out\n",
    "    *   C) That the same keys end up in the same partition\n",
    "    *   D) That the data is redistributed evenly\n",
    "\n",
    "8.  What does **\"hash aggregate\"** involve in a group by operation?\n",
    "    *   A) Shuffling all of the data before counting\n",
    "    *   B) Doing a local count before shuffling the data\n",
    "    *   C) Sorting the data\n",
    "    *   D) Filtering null values\n",
    "\n",
    "9.  According to the video, why does Spark add an additional filter step even if filters are pushed down to the source?\n",
    "    *   A) To make the query plan more complex\n",
    "    *   B) To operate on a larger dataset\n",
    "    *   C) To guarantee correctness\n",
    "    *   D) To avoid predicate pushdown\n",
    "\n",
    "10. In which of the following scenarios might **predicate pushdown NOT work**?\n",
    "    *   A) When filtering on an integer column\n",
    "    *   B) When filtering on a string column\n",
    "    *   C) When the column is a map type\n",
    "    *   D) When filtering on a column that is not null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers :\n",
    "Here are the answers to the multiple-choice questions, with explanations:\n",
    "\n",
    "1.  B) Checking the syntax of the code. **Spark first checks if the syntax is correct** before generating any plan.\n",
    "2.  C) To optimize the logical plan. The Catalyst Optimizer's main function is to **optimize the logical plan** by applying techniques like filter and projection pushdown.\n",
    "3.  C) Adding a new column. Narrow transformations don't involve shuffling. Adding a column is a narrow transformation, whereas repartitioning involves a shuffle and is thus a wide transformation.\n",
    "4.  B) Pushes filters to the data source. Filter push down is an optimization technique that **pushes filter operations to the data source** to reduce the amount of data read.\n",
    "5.  C) It always involves a partitioning scheme. **Coalesce tries to avoid shuffles** and thus doesn't always have a partitioning scheme. Repartitioning always involves a shuffle.\n",
    "6.  C) Runtime statistics. AQE uses runtime statistics such as number of byte rate and number of partitions to choose the most efficient query plan.\n",
    "7.  C) That the same keys end up in the same partition. Hash partitioning ensures that rows with **the same key (e.g., join key)** end up in the same partition after a shuffle.\n",
    "8.  B) Doing a local count before shuffling the data. Hash aggregate first performs a **partial count locally** on each partition before the shuffle.\n",
    "9.  C) To guarantee correctness. Spark adds an additional filter as a **fail-safe** to ensure data integrity and correctness, because some filters cannot be pushed down.\n",
    "10. C) When the column is a map type. Predicate pushdown might not work when filtering on certain data types or unsupported expressions. An example provided in the video is filtering on a **map type column**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
